spark2-submit --master yarn --deploy-mode cluster --conf spark.yarn.queue=NONP.HAASBAP00100_10215 --conf spark.executor.instances=22 --conf spark.executor.cores=4 --conf spark.executor.memory=20g --conf spark.sql.shuffle.partitions=176 --driver-memory=6g --conf spark.shuffle.file.buffer=64k --conf spark.reducer.maxSizeInFlight=96m --conf spark.shuffle.io.maxRetries=40 --conf spark.shuffle.io.retryWait=40 --conf spark.shuffle.spill.compress=true --conf spark.shuffle.compress=true --conf spark.maxRemoteBlockSizeFetchToMem=4g  --conf spark.kryoserializer.buffer.max=120m --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+G1SummarizeConcMark -Xms20G -XX:ConcGCThreads=10 -XX:InitiatingHeapOccupancyPercent=30 -XX:MaxGCPauseMillis=100" --class com.bt.ltok.sld.indicators.driver.LtokSldIndicators LTOK-SLD-INDICATORS-0.0.1.jar /user/HAASBAP00100_10215/LTOK_HAAS_DATA_PATH_POC/ltok_sld_indicators.properties




import org.apache.spark.sql.types.DataTypes._
import scala.collection.mutable.ArrayBuffer
val df = sc.parallelize(Seq(("1", "", "", ""), ("", "2", "", ""))).toDF("c1", "c2", "c3", "c4")
val modifiedDf = df.select(df.columns.map(columnName => col(columnName).cast("int").as(columnName)): _*)
val fillDf = modifiedDf.na.fill(0)
val sumDf = fillDf.select(fillDf.columns.map(columnName => max(col(columnName)).as(columnName)):_*)
val dropColumn = ArrayBuffer[String]()
sumDf.columns.foreach{ column =>
if(sumDf.select(col(column).cast(LongType)).first.getLong(0) == 0)
dropColumn += column
}
modifiedDf.drop(dropColumn:_*)


useful links:
https://github.com/iqblmoh/machineCoding
https://workat.tech/machine-coding/article/how-to-practice-for-machine-coding-kp0oj3sw2jca
